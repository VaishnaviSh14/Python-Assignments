{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070ded6a-aff4-4f65-abdb-438762c7a7c4",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "Ans.\n",
    "Web scraping is the automated process of extracting data from websites. It involves using software tools or scripts to access web pages, parse the HTML or other structured data, and extract the desired information. Web scraping is commonly used to gather large amounts of data from various websites quickly and efficiently.\n",
    "\n",
    "Three areas are -\n",
    "1.Data collection\n",
    "2.Business Intelligence\n",
    "3.Financial analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a436e1-8b02-47bb-a65b-de7c6cfc2757",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?\n",
    "Ans.\n",
    "different methods used for web scraping:\n",
    "\n",
    "1. HTTP Libraries\n",
    "2. HTML Parsing Libraries\n",
    "3. XPath\n",
    "4. CSS Selectors\n",
    "5. Regular Expressions\n",
    "6. Headless Browsers\n",
    "7. APIs\n",
    "8. Scraping Frameworks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16cc9c-222f-4ca3-95cc-3ef485fb21c3",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?\n",
    "Ans.\n",
    "Beautiful Soup is a Python library used for web scraping. It provides tools for extracting data from HTML and XML documents, navigating the parse tree (DOM), and searching for specific elements based on tags, attributes, and text content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5438a77-6418-4df2-a435-e589672dd25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Why is flask used in this Web Scraping project?\n",
    "Ans.\n",
    "Flas is used because of these features - \n",
    "Web Application Development\n",
    "Data Visualization\n",
    "RESTful APIs\n",
    "Task Scheduling\n",
    "Authentication and Access Control\n",
    "Error Handling and Logging\n",
    "Integration with Python Libraries\n",
    "Scalability\n",
    "Lightweight and Flexible\n",
    "Strong Community and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8773ad-f173-4ae3-ae73-1df1983f8a06",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "Ans.\n",
    "1. Elastic Beanstalk:\n",
    "\n",
    "Elastic Beanstalk is an Amazon Web Services (AWS) service that simplifies the process of deploying, managing, and scaling web applications and services. It abstracts away the complexity of infrastructure management, allowing developers to focus on writing code and uploading their applications. When used in web scraping projects, Elastic Beanstalk streamlines the deployment process, automatically handling capacity provisioning, load balancing, auto-scaling, and application health monitoring. It ensures that the application can efficiently handle varying workloads by automatically scaling based on incoming traffic. Load balancing capabilities distribute incoming requests across multiple instances of the application, enhancing performance and availability. Moreover, Elastic Beanstalk facilitates environment management, allowing developers to easily create and manage different environments for their web scraping applications, such as development, staging, and production. Additionally, it integrates seamlessly with other AWS services, providing a comprehensive solution for web scraping projects.\n",
    "\n",
    "2. CodePipeline:\n",
    "\n",
    "AWS CodePipeline is a powerful continuous integration and continuous delivery (CI/CD) service that automates the build, test, and deployment processes for applications. It enables developers to define a workflow or pipeline for their code changes, starting from code commit to production deployment. In web scraping projects, CodePipeline plays a vital role in automating the delivery process. Whenever changes are pushed to the repository, the pipeline automatically triggers the build and testing of the web scraping application. This continuous integration ensures that the latest code changes are promptly integrated into the application, reducing the chances of manual errors. Moreover, the continuous delivery aspect of CodePipeline streamlines the release of new versions of the web scraping application, allowing for rapid and reliable deployment. CodePipeline seamlessly integrates with other AWS services, such as Elastic Beanstalk, CodeBuild, and CodeDeploy, enabling developers to create a comprehensive CI/CD pipeline for their web scraping projects. Additionally, it works with various version control systems, including GitHub, Bitbucket, and AWS CodeCommit, making it versatile and adaptable to different development workflows.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
